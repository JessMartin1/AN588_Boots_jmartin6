---
title: "weavers_PeerCommentary_jmartin6_05"
author: "Sofia M. Weaver"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

Bootstrapping Standard Errors and CIs for Linear Models: When we initially discussed the central limit theorem and confidence intervals, we showed how we could use bootstrapping to estimate standard errors and confidence intervals around certain parameter values, like the mean. Using bootstrapping, we could also do the same for estimating standard errors and CIs around regression parameters, such as β coefficients.

# Question #1 
Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).

```{r, Dataset}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

```{r, Question #1}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean))
summary(m)
```

```{r, Question #1, coefficients}
m$coefficients
```

```{r, Question #1, CI}
ci <- confint(m, level = 0.95)  # Using the results of lm()
ci
```

*NOTE FROM SOFIA*
This looks great! Our code is pretty much identical. I initially tried to make the log transformed variables into objects beforehand and I got the same values but it got a little cranky later on when I tried to use the object again so I just left it as log(). 

# Question #2

Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.

**Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.**

```{r, Question #2}
library(boot)
set.seed(123)
# Function to obtain regression weights
bs <- function(formula, data, indices)
{
  d <- data[indices,] # allows boot to select sample
  fit <- lm(formula, data=d)
  return(coef(fit))
}
```

```{r, Bootstrapping}
# Bootstrapping with 1000 replications
results <- boot(data=d, statistic=bs, R=1000, formula=log(HomeRange_km2) ~ log(Body_mass_female_mean))
```

```{r, Results}
# View results
print(results)
```

```{r, 95% CIs}
# Get 95% confidence intervals for each coefficient
boot.ci(results, type="bca", index=1)
boot.ci(results, type="bca", index=2)
```

*NOTE FROM SOFIA*
Again, our code is pretty much identical wich makes sense since we used the same reference site lol. 
Great job! This homework was pretty straight forward thankfully. :)

**How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?**

-The SE of the β coeffiecients from the former model (linear regression, using lm() function) was 0.67293 and 0.08488.

-The SE of the β coeffiecients from the latter model (bootstrapping) was 0.56620314 and 0.07340095.

-For both the β coefficients, the standard errors obtained using bootstrapping are slightly lower than those obtained using the lm() function.

**How does the latter compare to the 95% CI estimated from your entire dataset?**

-The linear regression model produced a 95% CI of -10.7720889 to -8.110374 (intercept) and 0.8685707 to 1.204292 (slope).

-The bootstrapping model produced a 95% CI of -10.642 to -8.350 (intercept) and 0.908 to 1.196 (slope).

-The 95% CIs are extremely similar, with the bootstrapping model being slightly narrower than the linear regression model.

# Challenges: Homework #5

* This homework assignment presented no challenges worth noting. I used Module 12 (Introduction to Linear Regression) and this [website](https://www.statmethods.net/advstats/bootstrapping.html) to assist me!